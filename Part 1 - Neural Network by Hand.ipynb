{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network By Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to train a very basic neural network \"by hand.\" This will use the python package ``numpy`` for matrix/array computing. It has all the basic functionality that MATLAB has.\n",
    "\n",
    "This notebook is going to go through the calculus of training a network and everything will be done \"by hand\" in ``numpy``. There are libraries designed for making neural networks that will handle the calculus for us (``scikit-learn`` has lots of basic machine learning algorithms implemented to where you only need 3ish lines of code to train them; ``pytorch`` and ``tensorflow`` require you to know how to put the algorithms together - meaning, they're more flexible - while still handling all the calculus for you). The later notebooks will not include any calculus.\n",
    "\n",
    "The goal of the Boston House Prices dataset is to ingest 13 features about a house and the neighborhood the house is in, and to predict how much the house will cost. Since the price is a real number that can take on (almost) any value, this is a **regression** problem. We'll discuss the other main type of prediction, **classification**, later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # numpy for matrix/array computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston  # we're going to use scikit-learn ONLY for the Boston dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_boston()\n",
    "print(dataset[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use the notation ``(R, C)`` for the shape of arrays. That indicates a 2-dimensional array (i.e., a matrix) with ``R`` rows and ``C`` columns. I'll use ``(D,)`` for a 1-dimensional array (i.e., a vector) with ``D`` elements. The notation extends to higher-dimensional arrays, which we'll see later. The fancy lingo for N-dimensional arrays is **tensors**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n",
      "Data shape: (506, 13)\n",
      "Features: ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "Target shape: (506,)\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the format the data is in\n",
    "print(dataset.keys())\n",
    "data = dataset[\"data\"]  # np.ndarray - shape: (examples, features)\n",
    "target = dataset[\"target\"]  # np.ndarray - shape: (examples,)\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Features:\", dataset[\"feature_names\"])\n",
    "print(\"Target shape:\", target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's begin talking about machine learning.\n",
    "\n",
    "Let's see how well we can predict the price of houses based on the features in the Boston house dataset.\n",
    "For a specific house, let's call the features $x_0, x_1, \\dots, x_{12}$ and let's call the price $y$. (Sometimes, we'll use a dataset index, but we're omitting that until it's necessary.) We're going to try to learn how to predict $y$ as a weighted average of the features $x_i$. In other words,\n",
    "$$ \\hat{y} = w_{0} x_0 + w_1 x_1 + \\dots w_{12} + x_{12} $$\n",
    "where $\\hat{y}$ is our estimate for $y$ and the weights $w_i$ are twelve real numbers that we're going to find numerically.\n",
    "\n",
    "Two notational conveniences: First, we can write $\\hat{y}$ as a function of $x_i$ and $w_i$:\n",
    "$$ \\hat{y}(x_0, \\dots, x_{12}, w_0, \\dots, x_{12}) = w_0 x_0 + \\dots + w_{12} x_{12}.$$\n",
    "\n",
    "Second, the formula $\\hat{y} = \\sum_k w_k x_k$ is instantly recognizable as the formula for dot product, so we can actually simply write\n",
    "$$ \\hat{y} = \\vec{w} \\cdot \\vec{x}. $$\n",
    "\n",
    "Recall from linear algebra that we can express the dot product as a matrix multiplication. More on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data  # X matrix is all the data\n",
    "Y = target  # Y matrix is all the truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are we going to find the weights $w_i$? Well, more basic algorithms like least-squares work here, but let's introduce a concept called **gradient descent** that's much more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is very similar to least-squares regression. We're going to define an **error function** $E(y, \\hat{y})$, also sometimes called the **cost function** or the **loss function**, and we're going to try to minimize the sum (or average) of the error across the whole dataset, i.e. for all $y$. So far so good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive approach: Let's choose random numbers for $w_i$'s. Then, for each $i$, let's try to make it a little bigger. If the error goes down, good. Keep making that weight bigger. If the error goes up, oops. Make that weight smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues: That would take forever. And, we don't really know how much to make the weight bigger or smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: But wait a minute! What if I could tell you how much the error changes as each $w_i$ changes without having to change $w_i$? What if you could just find $E(y, \\hat{y})$ and automatically know which way to change $w_i$?\n",
    "\n",
    "This is sounding like a derivative..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as $E(y, \\hat{y}(x_0, \\dots, x_{12}, w_0, \\dots, w_{12}))$ has partial derivatives with respect to the weights $w_i$, we're good. We can tell immediately which way to change each weight when presented with $\\vec{x}$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this code is for the plot below, skip reading this and look at the plot\n",
    "\n",
    "import matplotlib.pyplot as plt  # plotting library for python\n",
    "\n",
    "w_i = np.linspace(0, 2)\n",
    "E = (w_i - 1) ** 2 + 1\n",
    "plt.plot(w_i, E)\n",
    "plt.xlabel(\"w_i\")\n",
    "plt.ylabel(\"Error as a function of w_i\")\n",
    "plt.title(\"Idea behind 1 Step of Gradient Descent\")\n",
    "\n",
    "current_w_i = 0.25  # a\n",
    "current_E = (current_w_i - 1) ** 2 + 1\n",
    "\n",
    "plt.scatter(current_w_i, current_E)\n",
    "plt.text(0.3, 1.55, \"Suppose w_i here gives this E\")\n",
    "\n",
    "plt.scatter([1], [1])  # plot the minimum\n",
    "plt.text(1, 1.05, \"Error minimized.\")\n",
    "\n",
    "# tangent line of E about the point w_i=a:\n",
    "# L(w_i) = f(a) + dE(a) \\times (w_i - a)\n",
    "w_i_local = np.linspace(0.1, 0.4)\n",
    "slope = 2 * (current_w_i - 1)\n",
    "line = current_E + slope * (w_i_local - current_w_i)\n",
    "plt.plot(w_i_local, line)\n",
    "\n",
    "_ = plt.text(0.4, 1.4, \"Increment w_i along line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[527.29820234]\n"
     ]
    }
   ],
   "source": [
    "# Let's get our initial random weights.\n",
    "w = np.random.randn(13, 1)  # (features, 1) weights arranged as a column vector\n",
    "\n",
    "def y_hat(x, w):\n",
    "    # x is a (1, 13) row vector\n",
    "    # w is a (13, 1) column vector\n",
    "    # we can find the weighted sum w0 x0 + ... + w12 x12 by matrix multiplying the vectors\n",
    "    return x @ w  # \"@\" is shorthand for matrix multiply in python. The result should be a 1x1 matrix\n",
    "\n",
    "estimate = y_hat(X[0, :], w)\n",
    "print(estimate)  # perfect, we got a number!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our error function is going to be the squared error function:\n",
    "$$ E(y, \\hat{y}) = (y - \\hat{y})^2 $$\n",
    "and when we sum the error across all $y$, we have (adding the dataset index $n$):\n",
    "$$ \\sum_{n=1}^N E(y_n, \\hat{y}_n) = \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 = \\text{SSE}.$$\n",
    "In other words, across the whole dataset, we are minimizing the sum-squared error.\n",
    "\n",
    "Let's give that a name: The total error function \n",
    "$$ E_T(Y, \\hat{Y}) = \\text{SSE} = \\sum_{n=1}^N (y_n - \\hat{y}_n)^2. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the calculus to find $\\frac{\\partial E_T}{\\partial w_i}$:\n",
    "\n",
    "Leibniz chain rule:\n",
    "$$ \\frac{\\partial E_T}{\\partial w_i} = \\frac{\\partial E_T}{\\partial \\hat{y}_k} \\frac{\\partial \\hat{y}_k}{\\partial w_i}. $$\n",
    "\n",
    "$$ \\frac{\\partial E_T}{\\partial \\hat{y}_k} = \\frac{\\partial}{\\partial \\hat{y}_n} ((y_0 - \\hat{y}_0)^2 + \\dots (y_k - \\hat{y}_k)^2 + \\dots + (y_N - \\hat{y}_N)^2) = \\frac{\\partial}{\\partial \\hat{y}_k} (y_k - \\hat{y}_k)^2 = 2 (y_k - \\hat{y}_k) \\times (-1) = 2 (\\hat{y}_k - y_k).$$\n",
    "$$ \\frac{\\partial \\hat{y}_k}{\\partial w_i} = x_{k_i}.$$\n",
    "So,\n",
    "$$ \\frac{\\partial E_T}{\\partial w_i} = 2 (\\hat{y}_k - y_k) \\times x_{k_i}.$$\n",
    "\n",
    "Note that our derivative implicitly depends on the dataset index $k$.\n",
    "We don't actually want that.\n",
    "We'll actually estimate $\\frac{\\partial E_T}{\\partial w_i}$ by averaging it across the entire dataset.\n",
    "But we will not write that detail in the math for notational convenience.\n",
    "\n",
    "The gradient of $E_T$ is given by:\n",
    "$$ \\nabla E_T = [\\frac{\\partial E_T}{\\partial w_0}, \\dots, \\frac{\\partial E_T}{\\partial w_{12}}].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are we going to update the weights?\n",
    "Well, let's try just changing them by how much they change the error. If a weight hurts the error a lot if we increase it ($\\frac{\\partial E_T}{\\partial w_i}$ is a large positive number), let's reduce $w_i$ quite a bit, etc. We're going to scale that by a factor of $\\eta$, called the **learning rate**.\n",
    "$$ w_i \\leftarrow w_i - \\eta \\frac{\\partial E_T}{\\partial w_i}.$$\n",
    "\n",
    "We can rewrite using linear algebra:\n",
    "$$ \\vec{w} \\leftarrow \\vec{w} - \\eta \\nabla E_T.$$\n",
    "This is the **update rule.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing X...\n",
      "Check minimum variance is far from 0: 0.013401098888632884\n",
      "Normalizing Y...\n",
      "Check minimum variance is far from 0: 84.41955615616556\n"
     ]
    }
   ],
   "source": [
    "# in practice, for technical reasons, it is VERY important to somehow normalize the different features.\n",
    "# There's different approaches:\n",
    "# zero-mean, unit-variance is common\n",
    "#     when we assume the values of each feature are normally distributed across the dataset\n",
    "# rescaling to a range of [-1, 1] is also common, but hurt badly by outliers.\n",
    "#\n",
    "# We're going to do zero-mean, unit variance. We'll take each feature column,\n",
    "# subtract it's mean, and divide by its standard deviation\n",
    "\n",
    "def zero_mean_unit_variance(D):\n",
    "    # D is a data matrix shaped (N_feats, ...)\n",
    "    feat_means = np.mean(D, axis=0)  # (N_feats,)-shaped vector\n",
    "    feat_vars = np.var(D, axis=0)  # (N_feats,)-shaped vector\n",
    "\n",
    "    # variance of 0 means an uninformative/constant feature.\n",
    "    # numerically, it's a problem b/c we can't divide by zero\n",
    "    try:\n",
    "        print(\"Check minimum variance is far from 0:\", min(feat_vars))\n",
    "    except Exception as e:\n",
    "        print(\"Check minimum variance is far from 0:\", feat_vars)\n",
    "\n",
    "\n",
    "    D_normed = (D - feat_means) / np.sqrt(feat_vars)\n",
    "    return D_normed, feat_means, feat_vars\n",
    "\n",
    "# keep means and vars to unnormalize, or to normalize new data the same way your weights are trained for\n",
    "print(\"Normalizing X...\")\n",
    "X_normed, X_means, X_vars = zero_mean_unit_variance(X)\n",
    "print(\"Normalizing Y...\")\n",
    "Y_normed, Y_mean, Y_var = zero_mean_unit_variance(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our First Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error after 100th update: [[286.47730386]]\n",
      "Error after 200th update: [[203.27167844]]\n",
      "Error after 300th update: [[171.20421651]]\n",
      "Error after 400th update: [[156.06559724]]\n",
      "Error after 500th update: [[147.966751]]\n",
      "Error after 600th update: [[143.1560225]]\n",
      "Error after 700th update: [[140.0352819]]\n",
      "Error after 800th update: [[137.87125248]]\n",
      "Error after 900th update: [[136.30125957]]\n",
      "Error after 1000th update: [[135.12965626]]\n",
      "Error after 1100th update: [[134.24064192]]\n",
      "Error after 1200th update: [[133.55957312]]\n",
      "Error after 1300th update: [[133.03498845]]\n",
      "Error after 1400th update: [[132.62971248]]\n",
      "Error after 1500th update: [[132.31608239]]\n",
      "Error after 1600th update: [[132.07314561]]\n",
      "Error after 1700th update: [[131.88486841]]\n",
      "Error after 1800th update: [[131.73890942]]\n",
      "Error after 1900th update: [[131.62573803]]\n",
      "Error after 2000th update: [[131.53798063]]\n"
     ]
    }
   ],
   "source": [
    "# (features, 1) weights arranged as a column vector, drawn from a standard normal distribution\n",
    "w = np.random.randn(13, 1)\n",
    "\n",
    "lr = .01  # learning rate eta\n",
    "\n",
    "def total_error(w, X_normed, Y_normed):\n",
    "    # keep in mind Y is normalized so this error is meaningless in the absolute sense.\n",
    "    # It's only useful relatively.\n",
    "    error_T = 0\n",
    "    for xk, yk in zip(X_normed, Y_normed):\n",
    "        xk = xk[np.newaxis, :]  # xk (num_feats,) -> (1,num_feats)\n",
    "        y_hat_k = y_hat(xk, w)\n",
    "        error = (y_hat_k - yk)**2\n",
    "        error_T += error\n",
    "    return error_T\n",
    "\n",
    "\n",
    "update_every = 100  # how many epochs do we want before printing some update about the error?\n",
    "\n",
    "\n",
    "N_epochs = 2000  # number of total passes through the data\n",
    "for epoch in range(1, N_epochs+1):\n",
    "    gradient_sum = np.zeros_like(w)\n",
    "    N = X_normed.shape[0]  # dataset size\n",
    "    for xk, yk in zip(X_normed, Y_normed):\n",
    "        # reshape the data to match the networks' expectations\n",
    "        xk = xk[np.newaxis, :]  # xk (num_feats,) -> (1,num_feats)\n",
    "        \n",
    "        # run the network: This is called the \"forward pass\".\n",
    "        y_hat_k = y_hat(xk, w)\n",
    "        \n",
    "        # accumulate the gradients.\n",
    "        grad_E_T_k = 2 * (y_hat_k - yk) * xk.T  # grad_E_T_k (num_feats,1)\n",
    "        gradient_sum += grad_E_T_k\n",
    "\n",
    "    # update the network: This is called the \"backward pass\"\n",
    "    gradient_average = gradient_sum / N\n",
    "    w = w - lr * gradient_average\n",
    "    \n",
    "    if epoch % update_every == 0:\n",
    "        print(f\"Error after {epoch}th update:\", total_error(w, X_normed, Y_normed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For house 1, true price is $21.6K\n",
      "We estimated $24.96082905537492K\n",
      "\n",
      "For house 2, true price is $34.7K\n",
      "We estimated $30.553738254477572K\n",
      "\n",
      "For house 3, true price is $33.4K\n",
      "We estimated $28.61396713526304K\n",
      "\n",
      "For house 4, true price is $36.2K\n",
      "We estimated $27.955750295148512K\n",
      "\n",
      "For house 5, true price is $28.7K\n",
      "We estimated $25.221351956742506K\n",
      "\n",
      "For house 6, true price is $22.9K\n",
      "We estimated $22.930951068414963K\n",
      "\n",
      "For house 7, true price is $27.1K\n",
      "We estimated $19.45451737777169K\n",
      "\n",
      "For house 8, true price is $16.5K\n",
      "We estimated $11.438377852391504K\n",
      "\n",
      "For house 9, true price is $18.9K\n",
      "We estimated $18.83090096056732K\n",
      "\n",
      "For house 10, true price is $15.0K\n",
      "We estimated $18.93316695577728K\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 10+1):\n",
    "    y_hat_k = y_hat(X_normed[k, :].T, w)\n",
    "    y_unnormed_hat_k = np.sqrt(Y_var) * (y_hat_k) + Y_mean\n",
    "    y_k = Y[k]\n",
    "\n",
    "    print(f\"For house {k}, true price is ${y_k}K\")\n",
    "    print(f\"We estimated ${y_unnormed_hat_k[0]}K\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices - Splitting the Data & Parallelizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We really want to know how our neural network did on data that it was never trained on, e.g. how well it generalizes. So, we should actually use a training and test split.\n",
    "\n",
    "For technical reasons, sometimes we want a third dataset called a validation dataset. We'll skip that here.\n",
    "This is a clear and concise explanation of the splits: https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7 (3 min)\n",
    "\n",
    "It's also important that we don't include the test set in the dataset normalization, because that's cheating :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(4242)  # set the random seed so this is reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check minimum variance is far from 0: 0.012945226004437496\n",
      "Check minimum variance is far from 0: 83.80700974999999\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data in the same way (that way the X and Y arrays are still parallel)\n",
    "data_index_array = list(range(X.shape[0]))\n",
    "random.shuffle(data_index_array)\n",
    "X_shuf = X[data_index_array]\n",
    "Y_shuf = Y[data_index_array]\n",
    "\n",
    "# normally you use about 80% for training and 20% for testing (which is what we're doing here).\n",
    "# There's about 500 houses in the dataset.\n",
    "# We're going to choose a random 400 for training and use the rest for testing.\n",
    "X_train = X[:400]  # since X, Y are shuffled, the first 400 are random.\n",
    "Y_train = Y[:400]\n",
    "\n",
    "X_test = X[400:]\n",
    "Y_test = Y[400:]\n",
    "\n",
    "X_norm_train, X_mean, X_var = zero_mean_unit_variance(X_train)\n",
    "X_norm_test = (X_test - X_mean) / np.sqrt(X_var)\n",
    "Y_norm_train, Y_mean, Y_var = zero_mean_unit_variance(Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate $\\hat{y}_k$ for more than one $x_k$ at a time. This is called **parallelization** or **batching**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_hat(X, w):\n",
    "    # X is a data array of shape (N_samples, N_feats)\n",
    "    # w is the weights vector of shape (N_feats, 1)\n",
    "    # Output Y_hat = X @ w  has shape (N_samples, 1) is the prediction for each data sample X\n",
    "    \n",
    "    # Recall we said x @ w is the dot product between x and w.\n",
    "    # Well, matrix multiplication is actually just parallel dot products\n",
    "    return X @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, the dataset is actually small enough we could actually calculate ``Y_hat`` across the entire dataset. Usually, this isn't the case. So, we'll \"mini-batch.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training error (real) after 100th update: $158.29k\n",
      "Mean testing error (real) after 100th update: $201.21k\n",
      "\n",
      "Mean training error (real) after 200th update: $93.15k\n",
      "Mean testing error (real) after 200th update: $96.30k\n",
      "\n",
      "Mean training error (real) after 300th update: $65.89k\n",
      "Mean testing error (real) after 300th update: $68.81k\n",
      "\n",
      "Mean training error (real) after 400th update: $50.55k\n",
      "Mean testing error (real) after 400th update: $57.11k\n",
      "\n",
      "Mean training error (real) after 500th update: $41.13k\n",
      "Mean testing error (real) after 500th update: $50.72k\n",
      "\n",
      "Mean training error (real) after 600th update: $35.07k\n",
      "Mean testing error (real) after 600th update: $46.64k\n",
      "\n",
      "Mean training error (real) after 700th update: $31.06k\n",
      "Mean testing error (real) after 700th update: $43.83k\n",
      "\n",
      "Mean training error (real) after 800th update: $28.36k\n",
      "Mean testing error (real) after 800th update: $41.83k\n",
      "\n",
      "Mean training error (real) after 900th update: $26.52k\n",
      "Mean testing error (real) after 900th update: $40.40k\n",
      "\n",
      "Mean training error (real) after 1000th update: $25.25k\n",
      "Mean testing error (real) after 1000th update: $39.38k\n",
      "\n",
      "Mean training error (real) after 1100th update: $24.37k\n",
      "Mean testing error (real) after 1100th update: $38.66k\n",
      "\n",
      "Mean training error (real) after 1200th update: $23.76k\n",
      "Mean testing error (real) after 1200th update: $38.17k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (features, 1) weights arranged as a column vector, drawn from a standard normal distribution\n",
    "w = np.random.randn(13, 1)\n",
    "\n",
    "lr = .005  # learning rate eta\n",
    "batch_size = 100\n",
    "\n",
    "def batchify(X, Y, n):\n",
    "    # this weird function is what's called a generator.\n",
    "    # Don't worry too much about it. Just know that we can write a for-loop\n",
    "    # that iterates over what this function returns.\n",
    "    for i in range(0, len(X), n):\n",
    "        yield X[i:i + n], Y[i:i+n]\n",
    "\n",
    "def total_error(w, X, Y, y_var=None, y_mean=None):\n",
    "    # If y_var and y_mean are provided, we'll go ahead and unnormalize\n",
    "    # to give a real absolute error.\n",
    "    error_T = 0\n",
    "    for X_batch, Y_batch in batchify(X, Y, batch_size):\n",
    "        Y_batch = Y_batch[:, np.newaxis]  # (num_samples, 1)\n",
    "        Y_hat = y_hat(X_batch, w)  # (num_samples, 1)\n",
    "        if y_var is not None:\n",
    "            Y_hat = Y_hat * np.sqrt(y_var) + y_mean\n",
    "        error = (Y_hat - Y_batch)**2  # (num_samples, 1)\n",
    "        error_t = np.sum(error)\n",
    "        error_T += error_t\n",
    "    return error_T\n",
    "\n",
    "\n",
    "update_every = 100\n",
    "\n",
    "\n",
    "N_epochs = 1200  # number of total passes through the data\n",
    "for epoch in range(1, N_epochs+1):\n",
    "    gradient_sum = np.zeros_like(w)\n",
    "    N = X_norm_train.shape[0]  # dataset size\n",
    "    for X_batch, Y_batch in batchify(X_norm_train, Y_norm_train, batch_size):\n",
    "        # X_batch (num_samples,num_feats)\n",
    "        Y_batch = Y_batch[:, np.newaxis]  # expand to (num_samples, 1)\n",
    "        \n",
    "        # forward pass\n",
    "        Y_hat = y_hat(X_batch, w)  # Y_hat (num_samples,1)\n",
    "        \n",
    "        # gradient accumulation\n",
    "        grad_E_T_batch = 2 * (Y_hat - Y_batch) * X_batch  # grad_E_T_k (num_samples, num_feats)\n",
    "        # reduce to (1,num_feats) and flip back to shape of w\n",
    "        gradient_sum += np.sum(grad_E_T_batch, axis=0, keepdims=True).T\n",
    "\n",
    "    # backwards pass\n",
    "    gradient_average = gradient_sum / N\n",
    "    w = w - lr * gradient_average\n",
    "    \n",
    "    if epoch % update_every == 0:\n",
    "        error = total_error(w, X_norm_train, Y_train, y_var=Y_var, y_mean=Y_mean)\n",
    "        mean_error = error / X_train.shape[0]\n",
    "        print(f\"Mean training error (real) after {epoch}th update:\", f\"${mean_error:.2f}k\")\n",
    "        \n",
    "        error = total_error(w, X_norm_test, Y_test, y_var=Y_var, y_mean=Y_mean)\n",
    "        mean_error = error / X_test.shape[0]\n",
    "        print(f\"Mean testing error (real) after {epoch}th update:\", f\"${mean_error:.2f}k\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For house 1, true price is $24.0K\n",
      "We estimated $12.99474267544698K\n",
      "\n",
      "For house 2, true price is $21.6K\n",
      "We estimated $20.060801342612976K\n",
      "\n",
      "For house 3, true price is $34.7K\n",
      "We estimated $21.088554112016904K\n",
      "\n",
      "For house 4, true price is $33.4K\n",
      "We estimated $12.85435516448003K\n",
      "\n",
      "For house 5, true price is $36.2K\n",
      "We estimated $7.1710974627689446K\n",
      "\n",
      "For house 6, true price is $28.7K\n",
      "We estimated $5.9086641290990265K\n",
      "\n",
      "For house 7, true price is $22.9K\n",
      "We estimated $4.904890362166736K\n",
      "\n",
      "For house 8, true price is $27.1K\n",
      "We estimated $20.121206910800716K\n",
      "\n",
      "For house 9, true price is $16.5K\n",
      "We estimated $13.547181082849228K\n",
      "\n",
      "For house 10, true price is $18.9K\n",
      "We estimated $22.13049980774101K\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = range(10)\n",
    "y_hat_k = y_hat(X_norm_test[samples, :], w)\n",
    "y_unnormed_hat_k = np.sqrt(Y_var) * (y_hat_k) + Y_mean\n",
    "\n",
    "for k in samples:\n",
    "    print(f\"For house {k+1}, true price is ${Y[k]}K\")\n",
    "    print(f\"We estimated ${y_unnormed_hat_k[k][0]}K\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Can We Do Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, our results still aren't very good. How can we make our network better?\n",
    "\n",
    "One simple idea is adding a **bias**, or turning our network into an *affine transformation* instead of a linear one.\n",
    "For example,\n",
    "$$ \\hat{y} = w_{0} x_0 + w_1 x_1 + \\dots w_{12} + x_{12} + b$$\n",
    "where $b$ is a real number.\n",
    "\n",
    "Since we're normalizing $Y$ such that its distribution has zero mean, in theory this won't have much of an effect.\n",
    "But, in practice, it often helps.\n",
    "\n",
    "Note that we'll be optimizing $b$, so we'll need the derivative of error with respect to $b$:\n",
    "$$ \\frac{\\partial \\hat{y}_k}{\\partial b} = 1, $$\n",
    "$$ \\frac{\\partial E_T}{\\partial b} = \\frac{\\partial E_T}{\\partial \\hat{y}_k} \\frac{\\partial \\hat{y}_k}{\\partial b}  = \\frac{\\partial E_T}{\\partial \\hat{y}_k} \\times 1 = 2 (\\hat{y}_k - y_k). $$\n",
    "\n",
    "Since the network is getting a little more complicated, let's move to an object-oriented approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    def __init__(self, num_features):\n",
    "        self.W = np.random.randn(num_features, 1)\n",
    "        self.b = np.zeros((1, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # compute the prediction, i.e. perform the \"forward pass\".\n",
    "        # x is shaped (num_examples, num_features)\n",
    "        # y_hat is shaped (num_examples, 1)\n",
    "        y_hat = x @ self.W + self.b\n",
    "        return y_hat\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # ``__call__`` is a \"magic method\" in Python. If you define a ``__call__``,\n",
    "        # your object becomes \"callable\" - you can use it like a function.\n",
    "        #\n",
    "        # net = Net()\n",
    "        # y = net(x)\n",
    "        #\n",
    "        # This might seem pointless here. We're doing it because it makes our class\n",
    "        # have similar syntax to what PyTorch uses.\n",
    "        return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we're going to introduce **minibatch stochastic gradient descent.** Whereas before, we were accumulating the gradients across many minibatches, this time we will speed up learning by updating the network after each minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training error (real) after 200th update: $178.68k\n",
      "Mean testing error (real) after 200th update: $275.12k\n",
      "\n",
      "Mean training error (real) after 400th update: $122.83k\n",
      "Mean testing error (real) after 400th update: $204.08k\n",
      "\n",
      "Mean training error (real) after 600th update: $93.46k\n",
      "Mean testing error (real) after 600th update: $156.29k\n",
      "\n",
      "Mean training error (real) after 800th update: $75.02k\n",
      "Mean testing error (real) after 800th update: $119.42k\n",
      "\n",
      "Mean training error (real) after 1000th update: $62.58k\n",
      "Mean testing error (real) after 1000th update: $92.24k\n",
      "\n",
      "Mean training error (real) after 1200th update: $53.75k\n",
      "Mean testing error (real) after 1200th update: $72.69k\n",
      "\n",
      "Mean training error (real) after 1400th update: $47.26k\n",
      "Mean testing error (real) after 1400th update: $58.88k\n",
      "\n",
      "Mean training error (real) after 1600th update: $42.36k\n",
      "Mean testing error (real) after 1600th update: $49.26k\n",
      "\n",
      "Mean training error (real) after 1800th update: $38.59k\n",
      "Mean testing error (real) after 1800th update: $42.67k\n",
      "\n",
      "Mean training error (real) after 2000th update: $35.62k\n",
      "Mean testing error (real) after 2000th update: $38.24k\n",
      "\n",
      "Mean training error (real) after 2200th update: $33.27k\n",
      "Mean testing error (real) after 2200th update: $35.34k\n",
      "\n",
      "Mean training error (real) after 2400th update: $31.39k\n",
      "Mean testing error (real) after 2400th update: $33.51k\n",
      "\n",
      "Mean training error (real) after 2600th update: $29.86k\n",
      "Mean testing error (real) after 2600th update: $32.43k\n",
      "\n",
      "Mean training error (real) after 2800th update: $28.61k\n",
      "Mean testing error (real) after 2800th update: $31.87k\n",
      "\n",
      "Mean training error (real) after 3000th update: $27.59k\n",
      "Mean testing error (real) after 3000th update: $31.67k\n",
      "\n",
      "Mean training error (real) after 3200th update: $26.74k\n",
      "Mean testing error (real) after 3200th update: $31.71k\n",
      "\n",
      "Mean training error (real) after 3400th update: $26.04k\n",
      "Mean testing error (real) after 3400th update: $31.91k\n",
      "\n",
      "Mean training error (real) after 3600th update: $25.46k\n",
      "Mean testing error (real) after 3600th update: $32.20k\n",
      "\n",
      "Mean training error (real) after 3800th update: $24.97k\n",
      "Mean testing error (real) after 3800th update: $32.56k\n",
      "\n",
      "Mean training error (real) after 4000th update: $24.56k\n",
      "Mean testing error (real) after 4000th update: $32.96k\n",
      "\n",
      "Mean training error (real) after 4200th update: $24.22k\n",
      "Mean testing error (real) after 4200th update: $33.36k\n",
      "\n",
      "Mean training error (real) after 4400th update: $23.93k\n",
      "Mean testing error (real) after 4400th update: $33.76k\n",
      "\n",
      "Mean training error (real) after 4600th update: $23.68k\n",
      "Mean testing error (real) after 4600th update: $34.15k\n",
      "\n",
      "Mean training error (real) after 4800th update: $23.48k\n",
      "Mean testing error (real) after 4800th update: $34.52k\n",
      "\n",
      "Mean training error (real) after 5000th update: $23.30k\n",
      "Mean testing error (real) after 5000th update: $34.87k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = Net(13)  # instantiate our class\n",
    "\n",
    "lr = .001  # learning rate eta  - since we're updating per batch, we'll want to lower this.\n",
    "# that's because each update could be somehow more biased or overall less complete\n",
    "batch_size = 100\n",
    "\n",
    "def batchify(X, Y, n):\n",
    "    for i in range(0, len(X), n):\n",
    "        yield X[i:i + n], Y[i:i+n]\n",
    "\n",
    "def total_error(net, X, Y, y_var=None, y_mean=None):\n",
    "    error_T = 0\n",
    "    for X_batch, Y_batch in batchify(X, Y, batch_size):\n",
    "        Y_batch = Y_batch[:, np.newaxis]  # (num_samples, 1)\n",
    "        Y_hat = net(X_batch)  # (num_samples, 1)\n",
    "        if y_var is not None:\n",
    "            Y_hat = Y_hat * np.sqrt(y_var) + y_mean\n",
    "        error = (Y_hat - Y_batch)**2  # (num_samples, 1)\n",
    "        error_t = np.sum(error)\n",
    "        error_T += error_t\n",
    "    return error_T\n",
    "\n",
    "\n",
    "update_every = 200\n",
    "\n",
    "batch_ctr = 0\n",
    "\n",
    "N_epochs = 1250  # number of total passes through the data\n",
    "for epoch in range(1, N_epochs+1):\n",
    "    # N = X_norm_train.shape[0]  # dataset size\n",
    "    for X_batch, Y_batch in batchify(X_norm_train, Y_norm_train, batch_size):\n",
    "        # X_batch (num_samples,num_feats)\n",
    "        Y_batch = Y_batch[:, np.newaxis]  # expand to (num_samples, 1)\n",
    "        \n",
    "        # forward pass\n",
    "        Y_hat = net(X_batch)  # Y_hat (num_samples,1)\n",
    "        \n",
    "        # gradients\n",
    "        grad_E_T_yk_batch = 2 * (Y_hat - Y_batch)  # grad_E_T_k (num_samples, num_feats)\n",
    "        \n",
    "        grad_E_T_b_batch = grad_E_T_yk_batch\n",
    "        grad_E_T_wk_batch = grad_E_T_yk_batch * X_batch\n",
    "        \n",
    "        this_batch_size = X_batch.shape[0]\n",
    "        grad_E_T_b_avg = np.sum(grad_E_T_b_batch, axis=0, keepdims=True).T / this_batch_size\n",
    "        # reduce to (1,num_feats) and flip back to shape of w\n",
    "        grad_E_T_wk_avg = np.sum(grad_E_T_wk_batch, axis=0, keepdims=True).T / this_batch_size\n",
    "        \n",
    "        # backwards pass\n",
    "        net.W -= lr * grad_E_T_wk_avg\n",
    "        net.b -= lr * grad_E_T_b_avg\n",
    "        \n",
    "        batch_ctr += 1\n",
    "    \n",
    "        if batch_ctr % update_every == 0:\n",
    "            error = total_error(net, X_norm_train, Y_train, y_var=Y_var, y_mean=Y_mean)\n",
    "            mean_error = error / X_train.shape[0]\n",
    "            print(f\"Mean training error (real) after {batch_ctr}th update:\", f\"${mean_error:.2f}k\")\n",
    "\n",
    "            error = total_error(net, X_norm_test, Y_test, y_var=Y_var, y_mean=Y_mean)\n",
    "            mean_error = error / X_test.shape[0]\n",
    "            print(f\"Mean testing error (real) after {batch_ctr}th update:\", f\"${mean_error:.2f}k\")\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For house 1, true price is $24.0K\n",
      "We estimated $12.549425458836106K\n",
      "\n",
      "For house 2, true price is $21.6K\n",
      "We estimated $19.54019003385459K\n",
      "\n",
      "For house 3, true price is $34.7K\n",
      "We estimated $20.54876642118027K\n",
      "\n",
      "For house 4, true price is $33.4K\n",
      "We estimated $13.352496722220797K\n",
      "\n",
      "For house 5, true price is $36.2K\n",
      "We estimated $7.57039492714031K\n",
      "\n",
      "For house 6, true price is $28.7K\n",
      "We estimated $5.801863018264328K\n",
      "\n",
      "For house 7, true price is $22.9K\n",
      "We estimated $7.991947394325848K\n",
      "\n",
      "For house 8, true price is $27.1K\n",
      "We estimated $21.906498498212308K\n",
      "\n",
      "For house 9, true price is $16.5K\n",
      "We estimated $15.417167230123763K\n",
      "\n",
      "For house 10, true price is $18.9K\n",
      "We estimated $23.211735139117447K\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = range(10)\n",
    "y_hat_k = net.forward(X_norm_test[samples, :])\n",
    "y_unnormed_hat_k = np.sqrt(Y_var) * (y_hat_k) + Y_mean\n",
    "\n",
    "for k in samples:\n",
    "    print(f\"For house {k+1}, true price is ${Y[k]}K\")\n",
    "    print(f\"We estimated ${y_unnormed_hat_k[k][0]}K\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following visualization is helpful for looking at the performance on the whole dataset all at once.\n",
    "The axial line represents a perfect classifier: The predicted price always equals the true price.\n",
    "Any deviation off of that line is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat_k = net(X_norm_test)  # run the net on the whole dataset\n",
    "y_unnormed_hat_k = np.sqrt(Y_var) * (y_hat_k) + Y_mean\n",
    "\n",
    "# the following is code for producing the plot.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(Y_test, y_unnormed_hat_k)\n",
    "plt.plot(np.linspace(0, 30), np.linspace(0, 30))\n",
    "plt.xlabel(\"True Price ($K)\")\n",
    "plt.ylabel(\"Predicted Price ($K)\")\n",
    "_ = plt.title(\"Test Set Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Limits of Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, linear models can only approximate linear functions.\n",
    "To give our neural network the ability to approximate nonlinear functions, we'll need to make its formula nonlinear somehow.\n",
    "\n",
    "And, more nonlinear than affine :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural networks can be used to approximate functions.\n",
    "* Neural networks are **parametric**: They have parameters (weights and biases) that have to be optimized.\n",
    "* The key algorithm for optimizing the weights and biases is called gradient descent.\n",
    "* Gradient descent works by incrementally adjusting the parameters according to the rate of change of the error with respect to the parameter. This means:\n",
    "    * The network must be \"differentiable\": You have to be able to find the (partial) derivative of the network's prediction with respect to each parameter.\n",
    "    * The error function must be differentiable: You have to be able to find the (partial) derivative of the error with respect to the network's prediction.\n",
    "* Linear neural networks can only approximate linear functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do On Your Owns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are cool, but if you're using a linear model, all this SGD stuff can become cumbersome. So can numpy (and even pytorch). Scikit-learn is great for doing data science at this level. Learn how to do this linear regression with scikit-learn:\n",
    "\n",
    "https://medium.com/@amitg0161/sklearn-linear-regression-tutorial-with-boston-house-dataset-cde74afd460a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
